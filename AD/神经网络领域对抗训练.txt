摘要Francois.Laviolette@ift.ulaval.ca Mario.Marchand@ift.ulaval.ca lempitsky@skoltech.ru我们引入了一种新的表示学习方法来进行领域自适应，其中训练和测试时间的数据来自相似但不同的分布。我们的方法直接受到领域适应理论的启发，该理论表明，要实现有效的域名转移，必须基于无法区分训练（源）域和测试（目标）域的特征进行预测。该方法在神经网络体系结构的上下文中实现了该思想，该神经网络体系结构对来自源域的标记数据和来自目标域的未标记数据进行了训练（不需要标记的目标域数据）。随着培训的进行，该方法促进了以下特征的出现：（i）区分源域上的主要学习任务，（ii）区分域之间的转移。我们证明这种适应行为 在几乎所有前馈模型中都可以通过增加很少的标准层和新的梯度反转层来实现。可以使用标准的反向传播和随机梯度下降来训练所得的增强架构，因此可以使用任何深度学习软件包轻松实现。我们展示了我们的方法针对两个不同的分类问题（文档情感分析和图像分类）的成功，该问题在标准基准上实现了最新的领域适应性能。我们还验证了在人员重新识别应用程序中描述子学习任务的方法。关键字：领域适应，神经网络，表示学习，深度学习，合成数据，图像分类，情感分析，人员重新识别。 

1.简介为新的机器学习任务生成标记数据的成本通常是应用机器学习方法的障碍。特别是，这是深度神经网络架构进一步发展的一个限制因素，深层神经网络架构已经在各种机器学习任务和应用程序中为最新技术带来了令人瞩目的进步。对于缺少标记数据的问题，仍然有可能获得足够大的训练集来训练大规模深度模型，但受数据分布与“测试时间”遇到的实际数据的影响而遭受损失的训练集。一个重要的例子是在合成或半合成图像上训练图像分类器，这些图像分类器可能数量很多并被完全标记，但是不可避免地具有与真实图像不同的分布（Liebelt和Schmid，2010； Stark等， 2010年； V́azquez等人，2014年； Sun和Saenko，2014年）。另一个例子是在情感分析中 书面评论，其中可能已经为一种产品（例如电影）的评论标记了数据，而又需要对其他产品（例如书籍）的评论进行分类。在训练分布和测试分布之间存在转换时学习判别式分类器或其他预测变量的方法称为领域适应（DA）。所提出的方法在源（训练时间）域和目标（测试时间）域之间建立映射，从而当与学习的域之间的映射组成时，为源域学习的分类器也可以应用于目标域。域自适应方法的吸引力在于，在目标域数据完全未标记（无监督域注释）或标记的样本很少（半监督域自适应）的情况下，学习域之间映射的能力。下面，我们将重点放在较难的无监督情况下，尽管可以将建议的方法（领域对抗学习）推广到 半监督的情况相当简单。与以前的许多有关领域适应的论文都使用固定特征表示方法不同，我们专注于在一个培训过程中将领域适应与深度特征学习相结合。我们的目标是将领域自适应嵌入学习表示的过程中，以便根据对领域变化具有区分性和不变性的特征做出最终的分类决策，即在源和源中具有相同或非常相似的分布目标域。这样，所获得的前馈网络可以适用于目标域，而不受两个域之间的移动的阻碍。我们的方法受域适应理论的启发（Ben-David等，2006，2010），该理论表明，跨域传输的良好表示是一种算法无法学习识别其来源域的方法。输入观察。因此，我们专注于学习结合 （i）判别力和（ii）域不变性。这是通过共同优化基础功能以及在这些功能上运行的两个判别式分类器来实现的：（i）预测类别标签并在训练和测试期间使用的标签预测器，以及（ii）区分培训期间的源域和目标域。在优化分类器的参数以最小化其在训练集上的误差的同时，优化基础深度特征映射的参数以最小化标签分类器的损失并最大化域分类器的损失。因此，后者的更新在领域分类器中是对抗性的，并且它鼓励在优化过程中出现领域不变特征。至关重要的是，我们证明了所有三个训练过程都可以嵌入到适当组合的深度前馈网络中，该网络称为领域对抗神经网络 （DANN）（如图1所示），它使用标准图层和损失函数，并且可以使用基于随机梯度下降或其修改（例如，带动量的SGD）的标准反向传播算法进行训练。该方法是通用的，因为可以为几乎任何可通过反向传播训练的现有前馈架构创建DANN版本。实际上，提出的体系结构的唯一非标准组件是相当平凡的梯度反转层，该层在正向传播期间保持输入不变，并在反向传播期间通过将其乘以负标量来反向梯度。我们在一系列深层架构和应用中对所提出的领域对抗性学习理念进行了实验评估。我们首先考虑最简单的DANN架构，其中树的各个部分（标签预测变量，领域分类器和领域预测变量）是线性的，并展示了针对此类体系结构的领域对抗学习的成功。 评估是针对合成数据以及自然语言处理中的情感分析问题进行的，其中DANN改善了Chen等人的最新边缘化堆叠自动编码器（mSDA）。 （2012）关于通用亚马逊评论基准。我们进一步评估了该方法在图像分类任务中的作用，并在MNIST（LeCun等，1998）和SVHN（Netzer等，2011）等传统深度学习图像数据集以及Office上提供了结果基准测试（Saenko et al。，2010），其中领域对抗学习允许获得深度架构，该架构大大提高了以前的最新准确性。最后，我们在人员重新识别应用程序的背景下评估域对抗描述符学习（Gong等人，2014），任务是获取适合检索和验证的良好行人图像描述符。我们应用领域对抗学习，因为我们认为训练的描述符预测变量 像暹罗语的损失，而不是经过分类损失训练的标签预测变量。在一系列实验中，我们证明了领域对抗学习可以大大改善跨数据集的重新识别。 

2.相关工作在许多方面探讨了实现领域适应的一般方法。多年来，大部分文献主要集中于线性假设（参见例如Blitzer等人，2006； Bruzzone和Marconcini，2010； Germain等人，2013； Baktashmotlagh等人，2013a； Cortes和Mohri ，2014）。最近，人们越来越多地研究非线性表示，包括神经网络表示（Glorot等，2011； Li等，2014），最著名的是最新的mSDA（Chen等，2012）。 ）。该文献主要关注基于去噪自动编码器范例的鲁棒表示原理（Vincent等，2008）。同时，已经提出了多种匹配源域和目标域中的特征分布的方法来进行无监督域自适应。一些方法通过重新称重或从源域中选择样本来执行此操作（Bor​​gwardt等，2006； Huang等，2006; Gong等人，2013），而其他人则寻求显式的特征空间变换，将源分布映射到目标对象中（Pan等人，2011； Gopalan等人，2011； Baktashmotlagh等人，2013b）。分布匹配方法的一个重要方面是测量分布之间（不相似）相似性的方法。在这里，一种流行的选择是在内核可复制的希尔伯特空间中匹配分布方式（Borgwardt等，2006； Huang等，2006），而Gong等。 （2012）；费尔南多等。 （2013年）绘制与每个分布关联的主轴。我们的方法还尝试匹配特征空间分布，但是这是通过修改特征表示本身而不是通过重新称重或几何变换来实现的。同样，我们的方法使用一种完全不同的方式，通过深度判别训练的分类器，基于分布的可分离性来测量分布之间的差异。另请注意，有几种方法可以执行 通过逐渐改变训练分布，从源域到目标域的逐步过渡（Gopalan等，2011； Gong等，2012）。在这些方法中，Chopra等人。 （2013年）通过对一系列深度自动编码器进行分层训练，以“深度”方式实现了这一目标，同时用目标域样本逐渐替换了源域样本。这比Glorot等人的类似方法有所改进。 （2011）只是为两个领域训练了一个单一的深度自动编码器。在两种方法中，使用自动编码器学习的特征表示在单独的步骤中学习实际的分类器/预测器。与Glorot等人相反。 （2011）； Chopra等。 （2013年），我们的方法在统一架构中使用单一学习算法（反向传播）联合执行特征学习，领域自适应和分类器学习。因此，我们认为我们的方法更简单（无论从概念上还是在实现方面）。我们的方法也取得了相当大的成就 在流行的Office基准上获得更好的结果。尽管上述方法执行无监督域自适应，但仍有一些方法通过利用来自目标域的标记数据来执行有监督域自适应。在深度前馈架构的背景下，此类数据可用于“微调”在源域上训练的网络（Zeiler和Fergus，2013； Oquab等，2014； Babenko等，2014）。我们的方法不需要标记的目标域数据。同时，它可以在可用时轻松合并这些数据。与我们有关的想法在Goodfellow等人中进行了描述。 （2014）。尽管他们的目标大相径庭（构建可以合成样本的生成型深度网络），但它们测量和最小化训练数据分布与合成数据分布之间差异的方式与我们的体系结构测量和最小化的方式非常相似两个域的特征分布之间的差异。 而且，作者提到由于域的显着差异，在训练的早期阶段可能会出现饱和的乙状结肠问题。他们用来解决此问题的技术（将梯度的“对抗性”部分替换为相对于适当成本计算出的梯度）可直接应用于我们的方法。另外，Tzeng等人的最新报告和并发报告。 （2014）； Long和Wang（2015）专注于前馈网络中的域适应。他们的一组技术可以测量并最小化跨域的数据分发手段之间的距离（可能是将分发嵌入RKHS之后）。因此，它们的方法与我们的匹配分布思想不同，因为它们使判别器无法区分。下面，我们将我们的方法与Tzeng等进行了比较。 （2014）； Long and Wang（2015）关于Office基准。已经开发出了另一种深层适应的方法，该方法与我们的方法有很大不同 在Chen等人中并行进行。 （2015）。从理论的角度来看，我们的方法直接来自于本戴维德等人的开创性的理论工作。 （2006，2010）。实际上，DANN直接优化了H-散度的概念。我们确实注意到Huang和Yates（2012）的工作，在该工作中，使用后正则化器学习了HMM表示法来进行单词标记，这也受到Ben-David等人的工作的启发。除了任务不同（Huang和Yates（2012）专注于单词标记问题）之外，我们还认为，DANN学习目标可以更紧密地优化H散度，而Huang和Yates（2012）出于效率原因依赖于更粗略的近似。本文的一部分已作为会议论文Ganin和Lempitsky（2015）出版。该版本通过合并报告Ajakan等人，极大地扩展了Ganin和Lempitsky（2015）。 （2014）（作为第二届“转移与多任务学习”研讨会的一部分），引入了新的术语， 对该方法进行了深入的理论分析和论证，使用浅色DANN案例对合成数据以及自然语言处理任务（情感分析）进行了广泛的实验。此外，在此版本中，我们超越了分类范围，并针对人员重新识别应用程序中的描述符学习设置评估了领域对抗学习。 

3.领域适应
我们考虑分类任务，其中X是输入空间，Y = {0，1，。 。 。 ，L-1}是
L个可能的标签集。此外，我们在X×Y上有两个不同的分布，称为
源域DS和目标域DT。无监督的领域适应学习
然后，向算法提供i.i.d绘制的标记源样本S。来自DS，还有一个
未标记的未标记目标样品T.来自DX，其中DX是TT的边际分布
DT超过X。
N = n n'是样本总数。学习算法的目标是
S = {（xi，yi）} ni = 1〜（DS）n; T = {xi} Ni = n 1〜（DTX）n'，从而建立具有低目标风险的分类器η：X→Y
RDT（η）= Pr􏱃η（x）̸=y􏱄，（x，y）〜DT
而没有有关DT标签的信息。
3.1域散度为了解决具有挑战性的域自适应任务，许多方法都将目标误差与源误差的总和以及源与目标分布之间的距离概念相结合。这些方法可以通过一个简单的假设直观地证明是正确的：当两种分布相似时，源风险可作为目标风险的良好指示。对于域适应，已经提出了几种距离的概念（Ben-David等，2006，2010； Mansour等，2009a，b； Germain等，2013）。在本文中，我们关注Ben-David等人使用的H-散度。 （2006年，2010年），并基于Kiferet等人的早期工作。 （2004）。请注意，我们在下面的定义1中假设假设类H是X上的DX和DX的1分布，并且假设类H在ST DX和DX之间的H-散度为ST􏱂􏱂STη∈H􏱂x〜DSX x〜DTX􏱂即，H散度取决于假设类别H的能力，以区分DX生成的示例与 DX生成的示例。本戴维（Ben-David）等。 ST（2006，2010）证明，对于对称假设类H，可以通过计算（离散或连续）集合来计算两个样本S〜（DSX）m和T〜（DTX）m'之间的经验H-散度。二进制分类器η：X→{0，1}。定义1（Ben-David et al。，2006，2010; Kifer et al。，2004）给定两个域􏱂􏰾􏰿􏰾􏰿􏱂dH（DX，DX）= 2 sup􏱂Prη（x）= 1-Prη （x）= 1􏱂。 􏱇􏱅1􏰁n1􏰁N􏱆􏱈dH（S，T）= 2 1−min I [η（xi）= 0]'I [η（xi）= 1]，（1）η∈Hni = 1 ni = n 1其中I [a]是指标函数，如果谓词a为true，则为1，否则为0。 3.2代理距离ˆ Ben-David等。 （2006年，2010年）建议，即使通常很难精确地计算dH（S，T）（例如，当H是X上的线性分类器的空间时），我们也可以通过在区分源示例和目标示例的问题。为此，我们构造了一个新的数据集U = {（（xi，0）} ni = 1∪{（xi，1）} Ni = n 1，（2）其中， 源样本标记为0，目标样本的示例标记为1。然后，在新数据集U上训练分类器的风险近似于等式（1）的“最小”部分。因此，给定关于区分源示例和目标示例的问题的测试误差ε，该代理A距离（PAD）由d dA = 2（1-2ε）给出。 （3）在本文的实验部分（请参阅第5.1.5节）中，我们按照Glorot等人的方法计算PAD值。 （2011）； Chen等。 （2012），即我们在U的一个子集上训练线性SVM（等式2），并在另一个子集上使用获得的分类器误差作为等式（3）中的ε值.3.3目标风险的广义约束本戴维等人的工作。 （2006年，2010年）还表明，H散度dH（DSX，DTX）ˆ定理2（Ben-David等，2006）令H为VC维d的假设类别。在样本S〜（DS）n和T〜（DTX）n的选择下，概率为−δ，对于每个η∈H：􏱍4􏰼2en4􏰽􏱍1􏰼2n4􏰽RDT（η）≤RS（ η） n dlog d logδdH（S，T）4 n dlog d logδδβ，β≥inf [RDS（η∗）RDT（η∗）]，η∗∈H由其经验估计dH（S ，T）加上取决于H的VC维度以及样本S和T的大小的恒定复杂度项。通过将此结果与源风险的相似范围相结合，可获得以下定理。 1􏰁mRS（η）= n I [η（xi）̸= yi] i = 1前一个结果告诉我们，只有当β项为低时，即只有当存在a时，RDT（η）才能为低。可以在两种分布上实现较低风险的分类器。它还告诉我们，要在给定的固定VC维类别中找到具有较小RDT（η）的分类器，学习算法应（在该类别中）最小化源风险ˆ RS（η）与经验H散度dH（S，T）。正如Ben-David等人指出的。 （2006年），控制H散度的策略是找到示例的表示形式，其中源域和目标域都尽可能地难以区分。 在定理2下，根据定理2，低源风险的假设在目标数据上表现良好。在本文中，我们提出了一种直接利用这一思想的算法。 

4.领域专家神经网络（DANN）
我们方法的原始方面是将定理2展示的思想明确地实现到神经网络分类器中。 也就是说，要学习一个可以从一个域很好地推广到另一个域的模型，我们确保神经网络的内部表示不包含有关输入来源（源或目标）的歧视性信息，同时保持较低的风险 在源（带有标签的）示例上。
在本节中，我们将详细介绍将“域适应组件”并入神经网络的建议方法。 在第4.1节中，我们首先针对最简单的情况（即单个隐藏层，完全连接的神经网络）提出想法。 然后，我们描述如何将方法推广到任意（深度）网络体系结构。

图1：建议的体系结构包括一个深层特征提取器（绿色）和一个深层标签预测器（蓝色），它们共同构成一个标准的前馈体系结构。 通过添加通过梯度反转层连接到特征提取器的域分类器（红色）来实现无监督的域自适应，该梯度反转层在基于反向传播的训练过程中将梯度乘以某个负常数。 否则，培训将按标准进行，并使标签预测损失（对于源示例）和域分类损失（对于所有样本）最小化。 梯度反转可确保使两个域上的特征分布相似（对于域分类器而言，尽可能不区分），从而导致域不变的特征。

5.实验
5.1浅层神经网络的实验
在第一个实验部分中，我们评估了4.1小节中描述的DANN简单版本的行为。 请注意，本小节中报告的结果是使用算法1获得的。基本上，这种随机梯度方法包括对一对源和目标示例进行采样并更新DANN所有参数的梯度步长更新。 至关重要的是，虽然常规参数的更新通常按照与梯度相反的方向进行，但对于对抗性参数，该步骤必须遵循梯度的方向（因为我们相对于梯度最大化，而不是最小化）。

5.1.1玩具问题的实验
作为第一个实验，我们研究了该算法在双绞卫星2D问题的变体上的行为，其中目标分布是源卫星的旋转。作为源样本S，我们生成分别标记为0和1的下月和上月，每个月包含150个示例。通过以下步骤获得目标样本T：（1）以与生成S相同的方式生成样本S'； （2）将每个示例旋转35°； （3）删除所有标签。因此，T包含300个未标记的示例。我们在图2中表示了这些示例。
我们通过将DANN与标准NN进行比较来研究其适应能力。在这些玩具实验中，两种算法共享相同的网络体系结构，隐藏层大小为15个神经元。我们甚至使用与DANN相同的步骤训练NN。也就是说，我们继续使用目标样本T（具有超参数λ= 6；对于DANN使用相同的值）来更新域回归分量，但是我们将对抗性反向传播禁用到隐藏层中。为此，我们通过省略第22和31行来执行算法1。这允许基于等式（5）的源风险最小化而无需任何正则化器的情况下恢复NN学习算法，并同时训练等式（7）的域回归区分源域和目标域。有了这种玩具经验，我们将首先说明DANN与NN相比如何适应其决策边界。此外，我们还将说明隐藏层给出的表示与DANN相比，与NN相比如何更不适合域任务（这就是为什么在NN实验中需要域回归器的原因）。我们记得这是我们提出的算法背后的基础思想。我们广泛的分析出现在图2中，其中上图与标准NN相关，下图与DANN相关。通过成对查看上下图，我们从四个不同的角度比较了NN和DANN，下面将详细介绍。

图2：相互缠绕的卫星玩具问题。 来自源样本的示例表示为“ +”（标签1）和“-”（标签0），而来自未标记目标样本的示例表示为黑点。

图2的“标签分类”列显示了DANN和NN在预测源示例和目标示例的标签问题上的决策边界。如预期的那样，NN可以对源样本S的两类进行准确分类，但不能完全适用于目标样本T。相反，DANN的决策边界可以完美地对源样本和目标样本进行分类。在研究的任务中，DANN明显适应目标分布。
列“表示PCA”研究域自适应正则化器如何影响网络隐藏层提供的表示Gf（·）。通过对主数据和目标数据点的所有表示形式（即S（Gf）∪T（Gf））的集合应用主成分分析（PCA）来获得这些图。因此，在给定训练网络（NN或DANN）的情况下，来自S和T的每个点都将通过隐藏层映射到15维特征空间，并通过PCA变换投影回二维平面。 PCA表示，我们观察到目标点在源点之间均匀分布；在NN-PCA表示中，目标点在没有源点所在的簇中进行了很大的分组。因此，考虑到DANN-PCA表示，标记目标点似乎是一件容易的事。
为了进一步推动分析，PCA图形通过字母A，B，C和D标记了四个关键数据点，这些数据点对应于原始空间中的月球末端（请注意，原始点的位置在第一列图形中标记了） 。我们注意到，在NN-PCA表示中，点A和B彼此非常接近，而它们显然属于不同的类。 C点和D点也一样。相反，这四个点位于DANN-PCA表示形式的相对的四个角上。还要注意，目标点A（resp。D）（很难在原始空间中分类）位于DANN-PCA表示形式的“ +”集群（resp。“-”集群）中。因此，DANN促进的表示更适合于适应问题。
“域分类”列示出了关于域分类问题的决策边界，其由等式（7）的域回归器Gd给出。更精确地，当Gd（Gf（x））≥0.5时，将示例x分类为源示例，否则将其分类为域示例。请记住，在DANN的学习过程中，Gd回归器难以区分源域和目标域，而隐藏表示Gf（·）会进行对抗性更新以防止其成功。如前所述，我们在NN的学习过程中训练了域回归器，但是不允许它影响学习的表示Gf（·）。
一方面，DANN域回归器完全无法泛化源和目标分布拓扑。另一方面，NN域回归器显示出更好的（尽管不完美）泛化能力。尤其是，它似乎大致捕获了目标分布的旋转角度。这再次证实了DANN表示不允许区分域。
“隐藏的神经元”列显示了隐藏层神经元的配置（通过等式4，我们认为每个神经元确实是线性回归器）。换句话说，对于i∈{1，，十五条绘图线的每条对应于坐标x∈R2，Gf（x）的第i个分量等于12。 。 。 ，15}。我们观察到，标准NN神经元分为三个簇，每个簇都允许为标签分类问题生成锯齿形决策边界的直线。但是，大多数这些神经元也能够（大致）捕获域分类问题的旋转角度。因此，我们观察到，DANN的自适应正则化器阻止了这类神经元的产生。确实令人惊讶的是，NN神经元中的两个主要模式（即从左下到右上横越平面的两条平行线）在DANN神经元中消失了。

5.1.2无监督超参数选择
要执行无监督的域自适应，应提供一种以无监督的方式设置超参数（例如，域正则化参数λ，学习率，我们的方法的网络体系结构）的方法，即，不引用目标中的标记数据域。在5.1.3和5.1.4节的以下实验中，我们将使用Zhong等人提出的反向交叉验证方法的一种变体来选择每种算法的超参数。 （2010），我们称之为反向验证。
为了评估与超参数元组相关的反向验证风险，我们进行如下操作。给定标记的源样本S和未标记的目标样本T，我们将每个集合分为训练集（分别为S'和T'，包含原始示例的90％）和验证集（分别为SV和TV）。我们使用标记集S'和未标记目标集T'来学习分类器η。然后，使用相同的算法，我们使用自标记集{（x，η（x））}x∈T'和S'的未标记部分作为目标样本来学习逆分类器ηr。最后，在源样本的验证集SV上评估反向分类器ηr。然后，我们说分类器η具有RSV（ηr）的反向验证风险。使用多个超参数值重复此过程，并且所选分类器是反向验证风险最低的分类器。
请注意，当我们训练神经网络架构时，验证集SV也用作学习η期间的早期停止标准，而自标记验证集{{x，η（x））}x∈TV被用作学习ηr期间的早期停止标准。当我们使用网络η所学习的配置初始化反向分类器ηr的学习时，我们还观察到了更好的精度。

5.1.3情绪分析数据集的实验
现在，我们将建议的DANN算法的性能与标准神经网络进行比较，该标准神经网络具有一个由等式（5）描述的隐藏层（NN）和具有线性核的支持向量机（SVM）。我们比较了Chen等人预处理过的亚马逊评论数据集上的算法。 （2012）。该数据集包括四个域，每个域都包含对特定种类产品（书籍，DVD光盘，电子产品和厨房用具）的评论。评论被编码为5,000个字母组合和双字母组合的特征向量，标签为二进制：如果产品排名最高为3星，则为“ 0”；如果产品排名为4或5星，则为“ 1”。
我们执行十二个领域适应任务。给所有学习算法提供了2000个带标签的源示例和2000个未带标签的目标示例。然后，我们在单独的目标测试集中评估它们（在3000到6000个示例之间）。请注意，NN和SVM不会使用未标记的目标样本进行学习。
这是有关每种学习算法所用步骤的更多详细信息，这些方法导致了表1的经验结果。
•对于DANN算法，在10-2和1之间的9个值中以对数标度选择自适应参数λ。隐藏层大小l为50或100。最后，学习率10−3
•对于NN算法，我们使用与上面的DANN完全相同的超参数网格和训练过程，只是我们不需要自适应参数。请注意，可以使用λ= 0的DANN实现（算法1）来训练NN。
•对于SVM算法，在对数刻度上的10−5和1之间的10个值中选择超参数C。此值范围与Chen等人使用的相同。 （2012）在他们的实验中。
如第5.1.2节所述，我们使用反向交叉验证为所有三种学习算法选择了超参数，并以早期停止作为DANN和NN的停止标准。
表1a的“原始数据”部分显示了所有算法的目标测试准确性，表1b报告了根据泊松二项式检验（Lacoste等人，2012）一种算法明显优于另一种算法的概率。 我们注意到DANN的性能明显优于NN和SVM，概率分别为0.87和0.83。 由于DANN和NN之间的唯一区别是域自适应调节器，因此我们得出结论，我们的方法成功地帮助找到了适合目标域的表示形式。

5.1.4将DANN与去噪自动编码器结合
现在，我们想知道我们的DANN算法是否可以改善Chen等人提出的最先进的边缘化堆叠式降噪自动编码器（mSDA）所学习的表示形式。 （2012）。简而言之，mSDA是一种无监督算法，可学习训练样本的新鲁棒特征表示。它需要源样本和目标样本的未标记部分来学习从输入空间X到新表示空间的特征图。作为一种降噪自动编码器算法，它可以找到一种特征表示形式，可以从该特征表示形式（近似）从嘈杂的对应形式中重建示例的原始特征。 Chen等。 （2012年）表明，将mSDA与线性SVM分类器结合使用可在Amazon Review数据集上达到最新的性能。作为SVM的替代方案，我们建议将浅DANN算法应用于mSDA生成的相同表示（使用源样本和目标样本的表示）。请注意，即使mSDA和DANN是两种表示学习方法，它们也会优化不同的目标，这些目标可以互补。
我们在前面小节中描述的同一亚马逊评论数据集上执行了该实验。对于每对源目标，我们使用50％的腐败概率和5的层数生成mSDA表示。然后，在这些表示上执行三种学习算法（DANN，NN和SVM）。更准确地说，遵循Chen等人的实验程序。 （2012），我们使用5层输出和原始输入的串联作为新表示。因此，现在每个示例都以30000维的向量进行编码。请注意，我们使用与之前的5.1.3小节相同的网格搜索，但对DANN和NN使用的学习率μ为10−4。表1a中“ mSDA表示形式”列的结果证实，结合使用mSDA和DANN是一种合理的方法。实际上，泊松二项式检验表明，与表1b相比，DANN具有比NN和SVM更好的性能，概率分别为0.92和0.88。

图3：代理A距离（PAD）。 请注意，交换源样本和目标样本时，mSDA表示的PAD值是对称的。

5.1.5代理A距离
DANN算法的理论基础是Ben-David等人的领域自适应理论。 （2006，2010）。我们声称，DANN找到了一个表示形式，在该表示形式中，源示例和目标示例几乎不可区分。我们在5.1.1节中进行的玩具实验已经指出了一些证据，但是我们想在真实数据上进行确认。为此，我们将Amazon Reviews数据集的各种表示形式上的代理A距离（PAD）进行了比较；这些表示是通过运行NN，DANN，mSDA或msDA和DANN组合获得的。回想一下，如3.2节所述，PAD是一种估计源和目标表示相似度的度量。更精确地，为了获得PAD值，我们使用以下过程：（1）我们使用训练样本的源表示形式和目标表示形式构造方程式（2）的数据集U； （2）我们将U随机分为两个大小相等的子集； （3）我们使用大范围的C值在U的第一个子集上训练线性SVM; （4）计算U的第二个子集上所有获得的分类器的误差； （5）使用最低误差来计算公式（3）的PAD值。
首先，图3a将在5.1.3节（使用导致表1结果的超参数值）的实验中获得的DANN表示的PAD与原始数据计算的PAD进行了比较。正如预期的那样，PAD值由DANN表示降低。
其次，图3b将DANN表示的PAD与标准NN表示的PAD进行了比较。由于PAD受隐藏层大小的影响（区分能力会随着表示长度的增加而增加），因此在这里，对于两种算法，我们将大小固定为100个神经元。我们还将DANN的自适应参数固定为λ≃0.31；它是我们在先前对Amazon Reviews数据集进行的实验中大多数时候选择的值。同样，DANN显然导致最低的PAD值。
最后，图3c给出了与5.1.4节实验相关的两组结果。一方面，我们再现了Chen等人的结果。 （2012年），注意到mSDA表示的PAD值比原始（原始）数据大。尽管mSDA方法显然有助于适应目标任务，但似乎与Ben-David等人的理论相矛盾。另一方面，我们观察到，当在mSDA之上运行DANN时（使用超参数值）导致表1）的结果，所获得的表示具有低得多的PAD值。这些观察结果可以解释DANN与mSDA程序结合使用时所提供的改进。

5.2使用深度网络进行图像分类的实验
现在，我们对许多流行的图像数据集及其修改形式对DANN的Deep版本（请参见第4.2节）进行广泛的评估。 这些包括受深度学习方法欢迎的小图像的大规模数据集，以及Office数据集（Saenko等人，2010），这是计算机视觉领域自适应的事实上的标准，但是图像却少得多。

5.2.1基准
在本小节实验中评估以下基线。只训练源模型，无需考虑目标域数据（网络中不包含域分类器分支）。在目标域上训练目标目标训练模型，并显示类别标签。该模型用作DA方法的上限，假设目标数据丰富且域之间的转移相当大。
此外，我们将我们的方法与最近提出的基于子空间对齐（SA）的无监督DA方法进行了比较（Fernando等人，2013），该方法易于在新数据集上进行设置和测试，但也显示出非常出色的性能与其他“浅” DA方法的实验比较很好。为了提高此基准的性能，我们从{2，...范围内选择其最重要的自由参数（主成分数）。 。 。 ，60}，以使目标域上的测试性能最大化。为了在我们的环境中应用SA，我们训练了仅源模型，然后将标签预测变量中最后一个隐藏层的激活（在最终线性分类器之前）视为描述符/特征，并了解源与目标之间的映射领域（Fernando et al。，2013）。由于SA基准要求在适应特征后需要训练新的分类器，并且为了将所有比较的设置放到平等的基础上，我们使用标准对标签预测子的最后一层进行重新训练所有四种考虑的方法（包括我们的方法）的线性SVM（Fan等人，2008）；在重新训练后，目标域的性能保持大致相同。
对于Office数据集（Saenko等人，2010），我们使用先前发布的结果直接比较了整个网络（功能提取器和标签预测器）与最近的DA方法的性能。

5.2.2 CNN架构和培训程序
通常，我们从两个或三个卷积层组成特征提取器，然后选择它们
先前作品的确切配置。更准确地说，四种不同的架构是
在我们的实验中使用。前三个如图5所示。对于Office域，我们
使用Caffe包装中经过预先训练的AlexNet（Jia等人，2014）。适应架构
3
除了MNIST，我们使用更简单的（x→100→2）架构来加快实验速度。我们认为，这些选择是任意的，如果对体系结构的这一部分进行了调整，则可能会获得更好的适应性能。
对于损失函数，我们将Ly和Ld分别设置为逻辑回归损失和二项式交叉熵。继Srivastava等。 （2014），当我们训练SVHN架构时，我们也使用dropout和l2- norm限制。
其他超参数没有通过网格搜索来选择，就像5.1节中的小规模实验一样，这在计算上是昂贵的。取而代之的是，在随机梯度下降过程中，使用以下公式调整学习率：
最后，请注意，该模型是在128个大小的批次上训练的（图像通过均值减法进行预处理）。 每个批次的一半填充有来自源域（带有已知标记）的样本，其余的包括目标域（具有未知标记）。

图5：实验中使用的CNN架构。 方框对应于应用于数据的转换。 颜色编码与图1相同。

5.2.3可视化
我们使用t-SNE（van der Maaten，2013）投影来可视化网络不同点的特征分布，同时对域进行颜色编码（图6）。 正如我们已经在DANN的浅表版本中观察到的那样（请参见图2），在根据目标域的分类准确性进行的适应成功与此类可视化中域分布之间的重叠之间存在强烈的对应关系。
5.2.4图像数据集的结果
现在，我们讨论实验设置和结果。 在每种情况下，我们都在源数据集上进行训练，并在不同的目标域数据集上进行测试，各域之间存在较大差异（请参见图4）。 结果总结在表2和表3中。

图6：适应对提取的特征的分布的影响（最佳观察颜色）。 该图显示了CNN活动的t-SNE可视化（van der Maaten，2013年）（a）未执行任何适应的情况，以及（b）将我们的适应程序纳入训练的情况。 蓝点对应于源域示例，而红点对应于目标域。 在所有情况下，我们方法中的调整都使特征的两种分布更加接近。

MNIST→MNIST-M。我们的第一个实验处理的是MNIST数据集（LeCun等，1998）（来源）。为了获得目标域（MNIST-M），我们将原始集合中的数字混合在从BSDS500的彩色照片中随机提取的补丁上（Arbelaezet等，2011）。正式为两个图像I1，I2定义此操作，因为Iout = | I1-I2 |，ijk ijk ijk
其中i，j是像素的坐标，k是通道索引。换句话说，通过从照片中获取补丁并将其像素反转到与数字像素相对应的位置来生成输出样本。对于人类而言，分类任务仅比原始数据集稍微困难一点（数字仍然可以清晰地区分），而对于在MNIST上训练的CNN来说，该域非常独特，因为背景和笔触不再恒定。因此，仅源模型的性能较差。我们的方法成功地对齐了特征分布（图6），这导致了成功的自适应结果（考虑到自适应是无监督的）。同时，通过子空间对齐（SA）实现的仅源模型的改进（Fernando等，2013）相当适度，因此突出了适应任务的难度。

合成数字→SVHN。为了解决训练综合数据和测试真实数据的常见情况，我们使用街景门牌号码数据集SVHN Netzeret等。 （2011）为目标域，以合成数字为源。后者（Syn Numbers）包含≈500,000张图像，这些图像是我们自己通过更改文本（包括不同的一位数，两位数和三位数），位置，方向，背景和笔触颜色以及数量从WindowsTM字体生成的的模糊。手动选择变化程度以模拟SVHN，但是两个数据集仍然相当不同，最大的不同是SVHN图像背景中的结构性杂波。
所提出的基于反向传播的技术很好地覆盖了仅使用源数据进行训练与使用已知目标标签进行目标域数据训练之间几乎80％的差距。相反，SA Fernando等人。 （2013年）导致分类精度略有下降（可能是由于降维过程中的信息丢失），表明适应任务比MNIST实验更具挑战性。

MNIST↔SVHN。在此实验中，我们进一步扩大了分布之间的差距，并在外观上有明显差异的MNIST和SVHN上进行了测试。即使没有适应性的SVHN培训也具有挑战性-在前150个时期中分类错误仍然很高。因此，为了避免以较差的局部最小值结束，我们在此不使用学习速率退火。显然，两个方向（MNIST→SVHN和SVHN→MNIST）并非同样困难。随着SVHN的多样性，期望在SVHN上训练的模型更加通用，并且可以在MNIST数据集上合理地执行。确实确实如此，并且由特征分布的外观支持。当我们将域馈入仅靠MNIST训练的CNN时，我们会发现域之间存在很强的分离，而对于SVHN训练的网络，其功能却更加混杂。这种差异可能可以解释为什么我们的方法通过在SVHN→MNIST场景（参见表2）中进行适配而成功地提高了性能，却没有相反的方向（在这种情况下，SA也无法执行适配）。从MNIST到SVHN的无监督适应为我们的方法提供了一个失败的例子（我们不知道任何能够执行这种适应的无监督DA方法）。

合成标志→GTSRB。总体而言，此设置类似于“ Syn Numbers→SVHN”实验，不同之处在于，由于类的数量大得多（43个而不是10个），功能的分布更加复杂。对于源域，我们获得了100,000个模拟各种成像条件的合成图像（我们称为Syn Signs）。在目标领域，我们使用31,367个随机训练样本进行无监督适应，其余样本进行评估。我们的方法再一次实现了性能上的显着提高，证明了其适用于从合成到实际的数据适配。
作为一项额外的实验，我们还评估了半监督域自适应的拟议算法，即当一个算法额外提供了少量标记目标数据时。在这里，我们揭示了430个带有标签的示例（每类10个样本），并将它们添加到标签预测变量的训练集中。图7显示了整个训练过程中验证错误的变化。虽然该图清楚地表明我们的方法在半监督设置中可能是有益的，但对半监督设置的全面验证留待以后的工作。

办公室数据集。最后，我们在Office数据集上评估了我们的方法，该数据集是三个不同域的集合：Amazon，DSLR和Webcam。与先前讨论的数据集不同，Office规模较小，仅2817个标签图像分布在最大域中的31个不同类别中。可用数据量对于成功训练深度模型至关重要，因此，我们选择了对在ImageNet上进行预训练的CNN（来自Caffe软件包的AlexNet（Jia等人，2014））进行微调。在最近的一些DA工作中已经完成（Donahue等人，2014; Tzeng等人，2014; Hoffman等人，2013; Long and Wang，2015）。我们使我们的方法与Tzeng等人更具可比性。 （2014年），使用完全相同的网络体系结构，将基于域均值的正则化替换为域分类器。
在先前的工作之后，我们将评估我们最常用的三个转移任务的性能。我们的训练规程取自Gong等人。 （2013）； Chopra等。 （2013）； Long和Wang（2015），因为在适应过程中，我们使用了所有可用的标记源示例和未标记目标示例（我们方法的前提是目标域中未标记数据的丰富性）。此外，所有源域都用于培训。在这种“全传导”设置下，我们的方法能够极大地提高先前报告的无监督适应的最新准确性（表3），尤其是在最具挑战性的Amazon→Webcam场景（两个领域最大的域偏移）。
有趣的是，在所有三个实验中，随着训练的进行，我们都发现有一些过拟合现象，但是，这并没有破坏验证的准确性。此外，关闭域分类器分支会使此效果更加明显，由此可以得出结论，我们的技术可以用作正则化器。

图7：半监督设置中交通标志分类的结果。 Syn和Real表示可用的标记数据（分别为100,000个合成图像和430个真实图像）； 适应意味着≈31,000个未标记的目标域图像用于适应。 通过在目标域中同时使用标记的样本和未标记的大型语料库，可以实现最佳性能。

5.3使用深层图像描述符进行重新识别的实验
在本节中，我们讨论所描述的适应方法在人员重新识别（re-id）问题上的应用。人员重新识别的任务是关联从不同摄像机视角看到的人员。更正式地讲，它可以定义如下：给定来自不同摄像机（探头和画廊）的两组图像，以使探针集中描绘的每个人在图库集中都有一个图像，对于探针集中一个人的每个图像在图库集中找到同一个人的图像。不连贯的相机视野，不同的照明条件，各种姿势以及低质量的数据，甚至对于人类来说，也很难解决这个问题（Liu等人，2013年，例如VIPeR上的Rank1 = 71.08％）。
与上面讨论的分类问题不同，重新识别问题意味着每个图像都映射到矢量描述符。然后使用描述符之间的距离来匹配探针集和图库集的图像。为了评估re-id方法的结果，通常使用累积匹配特征（CMC）曲线。它是等级k处的识别率（召回率）的图，即匹配画廊图像位于最接近探针图像的k张图像（根据描述符距离）之内的概率。
大多数现有作品训练描述符映射，并在包含来自具有类似成像条件的某个相机网络的图像的同一数据集中评估它们。但是，有几篇论文观察到，当描述符在一个数据集上训练而在另一数据集上进行测试时，所得重新识别系统的性能将大大下降。因此，将这样的跨域评估作为域适应问题来处理是很自然的，其中每个摄像机网络（数据集）都构成一个域。
最近，已经提出了几篇具有显着改善的重新识别性能的论文（Zhang和Saligrama，2014； Zhao等，2014； Paisitkriangkrai等，2015），Ma等。 （2015年）在跨数据集评估方案中报告了良好的结果。目前，深度学习方法（Yi等人，2014）可能无法获得最新的结果，这可能是因为训练集的规模有限。因此，域自适应代表了改善深度重新标识描述符的可行方向。

5.3.1数据集和协议
继马等。 （2015年），我们使用PRID（Hirzer等人），VIPER（Gray等人，2007），中大（Li和Wang，2013）作为我们实验的目标数据集。与Ma等人一样，PRID数据集有两种版本。 （2015），我们使用单次。它包含从相机A看的385人的图像和从相机B看的749人的图像，两个相机中都出现200人。 VIPeR数据集还包含用两台摄像机拍摄的图像，总共捕获了632个人，每个人对于两个摄像机视图中的每一个都有一个图像。香港中文大学的数据集包括来自5对摄像机的图像，两个摄像机每个人的两个图像。我们将此数据集的仅包括第一对相机的子集称为CUHK / p1（因为大多数论文都使用该子集）。
我们针对各种不同的数据集对进行了广泛的实验，其中一个数据集用作源域，即用于以监督的方式训练探针映射和画廊图像之间的已知对应关系的描述符映射。将第二个数据集用作目标域，以便使用该数据集中的图像而无需探针画廊对应。
更详细地，当CUHK作为目标域时，CUHK / p1用于实验；当CUHK作为源域时，两个设置（“整个CUHK”和CUHK / p1）用于实验。给定PRID作为目标数据集，我们随机选择在两个摄像机视图中都出现的100个人作为训练集。来自照相机A的其他100个人的图像用作探测，来自照相机B的所有图像（不包括训练中使用的图像（总计649））在测试时用作画廊。对于VIPeR，我们使用316名随机人员进行培训，并使用所有其他人员进行测试。中大将971人分为485人进行训练，而486人进行测试。与Ma等人不同。 （2015），我们使用中大第一对相机中的所有图像，而不是从每个相机视图中选择一个人的图像。与原始论文一样，我们还进行了两个实验，将整个中大数据集的所有图像作为源域，将VIPeR和PRID数据集作为目标域（Yi等人，2014）。
继易等。 （2014年），我们用镜像图像增强了数据，在测试期间，我们计算了两个图像之间的相似性得分，作为四个得分的平均值，这四个得分分别对应于两个比较图像的不同翻转。就中大而言，每个人的两个摄影机视图每个都有4张图像（包括镜像），则对所有16个组合的得分进行平均。

5.3.2 CNN架构和培训程序
在我们的实验中，我们使用Yi等人所述的暹罗架构。 （2014）（深度度量学习或DML），用于学习源数据集上的深度图像描述符。该架构包含两个卷积层（具有7×7和5×5滤波器组），然后是ReLU和最大池，以及一个完全连接的层，该层提供500维描述符作为输出。 CNN中有三个并行流，用于处理图像的三个部分：上部，中间和下部。第一卷积层在三个部分之间共享参数，第二卷积层的输出被级联。在训练期间，我们遵循Yi等。 （2014）并计算每个批次中500维特征之间的成对余弦相似度，并反向传播批次中所有对的损耗。
为了进行领域对抗训练，我们构建了DANN架构。特征提取器包括上面讨论的两个卷积层（后跟最大池化和ReLU）。在这种情况下，标签预测变量会替换为包含一个完全连接层的描述符预测变量。域分类器包括两个完全连接的层，中间表示为500个单位（x→500→1）。
对于描述符预测器中的验证损失函数，我们使用了Yi等人定义的二项式偏差损失。 （2014）使用相似的参数：α= 2，β= 0.5，c = 2（负对的不对称成本参数）。如5.2.2所述，通过逻辑损失训练域分类器。
我们使用固定为0.001和0.9的动量的学习率。使用了与5.2.2中描述的类似的时间表。在第二个最大池化层的输出串联之后，我们还以0.5的速率插入了辍学层。 128大小的批次用于源数据，而128大小的批次用于目标数据。
5.3.3重新识别数据集的结果
图9以CMC曲线的形式显示了八对数据集的结果。根据注释问题的难度，我们进行了50,000次迭代（CUHK / p1→VIPeR，VIPerR→CUHK / p1，PRID→VIPeR）或20,000次迭代（其他五对）。
经过足够数量的迭代后，领域对抗训练不断提高了重新识别的性能。对于涉及PRID数据集的对（与其他两个数据集更不相似），改进是可观的。总的来说，这证明了领域对抗学习除分类问题外的适用性。
图10进一步说明了适应对VIPeR→CUHK / p1实验中源和目标集中学习的描述符分布的影响。

六，结论
本文提出了一种前馈神经网络域自适应的新方法，该方法允许基于源域中的大量带注释数据和目标域中的大量未注释数据进行大规模训练。与许多以前的浅层和深层DA技术类似，通过在两个域之间对齐特征的分布来实现自适应。但是，与以前的方法不同，对齐是通过标准的反向传播训练来完成的。
该方法受到Ben-David等人的领域适应理论的激励和支持。 （2006，2010）。 DANN背后的主要思想是结合网络隐藏层来学习一种表示形式，该表示形式可以预测源示例标签，但不能提供有关输入域（源或目标）的信息。我们在浅层和深层前馈架构中都实现了这种新方法。后者允许通过引入简单的梯度反转层在几乎任何深度学习包中进行简单实现。我们已经表明，我们的方法是灵活的，并且可以在域适应的各种基准上获得最新的结果，即用于情感分析和图像分类任务。
我们方法的一个方便方面是，可以将域自适应组件添加到几乎任何可以反向传播训练的神经网络体系结构中。为此，我们已经通过实验证明了该方法不仅限于分类任务，而是可以用于其他前馈体系结构，例如用于描述者重新识别的描述符学习。

图9：在有和没有域对抗学习的情况下，VIPerR，PRID和CUHK / p1的结果。 跨八个领域对，领域对抗学习可提高重新识别的准确性。 对于某些域对，改进是可观的。

















